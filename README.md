# Large Language Model from Scratch

A simple and educational implementation of a decoder-only Transformer (GPT-style LLM) built from scratch.

---

## ğŸ“Œ Activation Function Comparison
<img src="https://raw.githubusercontent.com/shourya5204/Large-Language-Model-from-Scratch/main/assets/geluvsrelu.png" width="450">

## ğŸ“Œ GPT Model Stages
<img src="https://raw.githubusercontent.com/shourya5204/Large-Language-Model-from-Scratch/main/assets/gptmodel_stages.png" width="450">

## ğŸ“Œ Transformer Architecture
<img src="https://raw.githubusercontent.com/shourya5204/Large-Language-Model-from-Scratch/main/assets/transformer_arch.png" width="450">

---

## ğŸ“Œ What This Project Covers
- Basic tokenization  
- Embedding layer implementation  
- Self-attention and multi-head attention  
- Transformer decoder blocks  
- Training loop and loss calculation  
- Text generation using the trained model 

Everything is built step-by-step for clarity.

---

## â–¶ï¸ Run the Project

```bash
git clone https://github.com/shourya5204/Large-Language-Model-from-Scratch.git
cd Large-Language-Model-from-Scratch
Finally run the LLM_from_scratch,ipynb file on your own data
